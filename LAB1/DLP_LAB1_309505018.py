# -*- coding: utf-8 -*-
"""DLP_HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YMGxGHHa5eEzfeSweJY6X419cAY5LWgn
"""

# from google.colab import drive
# drive.mount('/content/gdrive')
# import sys
# sys.path.append('/content/gdrive/MyDrive/Colab/DLP_HW1')

import math
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def derivative_sigmoid(x):
  return x * (1 - x)

def generate_linear(n=100): #generate function given by TA
  pts = np.random.uniform(0,1,(n,2))
  inputs = []
  labels = []
  for pt in pts:
    inputs.append([pt[0],pt[1]])
    distance = (pt[0]-pt[1])/1.414
    if pt[0] > pt[1]:
      labels.append(0)
    else:
      labels.append(1)
  return np.array(inputs), np.array(labels).reshape(n,1)

def generate_XOR_easy(): #generate function given by TA
  inputs = []
  labels = []
  for i in range(100):
    inputs.append([0.1*i, 0.1*i])
    labels.append(0)
    if 0.1*i == 0.5:
      continue

    inputs.append([0.1*i, 1-0.1*i])
    labels.append(1)
  return np.array(inputs), np.array(labels).reshape(-1,1)

def show_result(x, y, pred_y): #show function given by TA
  plt.subplot(1,2,1)
  plt.title('Ground truth',fontsize=18)
  for i in range(x.shape[0]):
    if y[i] == 0:
      plt.plot(x[i][0], x[i][1], 'ro')
    else:
      plt.plot(x[i][0], x[i][1], 'bo')

  plt.subplot(1,2,2)
  plt.title('Predict result',fontsize=18)
  for i in range(x.shape[0]):
    if pred_y[i] == 0:
      plt.plot(x[i][0], x[i][1], 'ro')
    else:
      plt.plot(x[i][0], x[i][1], 'bo')

def cross_entropy(y,pred_y):
  loss=-(1/y.shape[1])*(y@np.log(pred_y+EPS).T+(1-y)@np.log(1-pred_y+EPS).T)
  return float(loss)

def forward(inputs):
  a[0]=inputs
  # hidden layer 1
  z[1]=W[1]@a[0]+b[1]
  a[1]=sigmoid(z[1])
  # hidden layer 2
  z[2]=W[2]@a[1]+b[2]
  a[2]=sigmoid(z[2])
  # output layer 
  z[3]=W[3]@a[2]+b[3]
  a[3]=sigmoid(z[3])
  return a[3]

def backward(gt_y,pred_y):
  batch_size=gt_y.shape[1]
  # bp
  grad_a3=-(gt_y/(pred_y+EPS)-(1-gt_y)/(1-pred_y+EPS))
  grad_z3=grad_a3*derivative_sigmoid(a[3])
  grad_W3=grad_z3@a[2].T*(1/batch_size)
  grad_b3=np.sum(grad_z3,axis=1,keepdims=True)*(1/batch_size)
          
  grad_a2=W[3].T@grad_z3
  grad_z2=grad_a2*derivative_sigmoid(a[2])
  grad_W2=grad_z2@a[1].T*(1/batch_size)
  grad_b2=np.sum(grad_z2,axis=1,keepdims=True)*(1/batch_size)
      
  grad_a1=W[2].T@grad_z2
  grad_z1=grad_a1*derivative_sigmoid(a[1])
  grad_W1=grad_z1@a[0].T*(1/batch_size)
  grad_b1=np.sum(grad_z1,axis=1,keepdims=True)*(1/batch_size)
          
  # update
  W[1]-=lr*grad_W1
  W[2]-=lr*grad_W2
  W[3]-=lr*grad_W3
  b[1]-=lr*grad_b1
  b[2]-=lr*grad_b2
  b[3]-=lr*grad_b3
          
  return
def train(X, y):
  lx = []
  ly = []
  # make sure that the amount of data and label is match
  assert X.shape[1] == y.shape[1]

  for epochs in range(num_step):
    # forward passing/ compute loss/ propagate gradient backward to the front
    pred_y=forward(X)
    backward(y,pred_y)
    lx.append(epochs)
    ly.append(cross_entropy(y,pred_y))
    if epochs % print_interval == 0:
      loss=cross_entropy(y,pred_y)
      acc=(1.-np.sum(np.abs(y-np.round(pred_y)))/y.shape[1])*100
      print(f'Epochs {epochs}: loss={loss:.5f} accuracy={acc:.2f}%')
    
  # plot learning curve
  plt.plot(lx,ly,'b-')
  plt.xlabel("epochs")
  plt.ylabel("loss")
  plt.title("generate_linear loss")
  plt.show()

def test(X, y):
  pred_y=forward(X)
  print(pred_y)
  loss=cross_entropy(y,pred_y)
  acc=(1.-np.sum(np.abs(y-np.round(pred_y)))/y.shape[1])*100
  print(f'loss={loss:.5f} accuracy={acc:.2f}%')

def plot_sigmoid():
  x = np.arange(-10., 10., 0.2)
  sig = sigmoid(x)
  dersig = derivative_sigmoid(sigmoid(x))
  plt.plot(x,sig,'b-',x,dersig,'r-')
  plt.show()

if __name__=='__main__':
  #### choose input datasets #########
  X, y = generate_linear(n=100)
  #X, y = generate_XOR_easy()
  ################# parameter define and initial ##################
  X=X.T
  y=y.T
  EPS=1e-3
  lr = 0.1
  num_step = 10000
  print_interval = 500
  hidden_size = (10,10)
  W=[None,np.random.randn(hidden_size[0],2),np.random.randn(hidden_size[1],hidden_size[0]),np.random.randn(1,hidden_size[1])]
  b=[None,np.zeros((hidden_size[0],1)),np.zeros((hidden_size[1],1)),np.zeros((1,1))]
  z=[None,np.zeros((hidden_size[0],1)),np.zeros((hidden_size[1],1)),np.zeros((1,1))]
  a=[None,np.zeros((hidden_size[0],1)),np.zeros((hidden_size[1],1)),np.zeros((1,1))]
  #########################################################################
  # train
  #plot_sigmoid()
  train(X,y)
  print('training finished\n')
  print('start testing:')
  test(X,y)
  print('testing finished')
  pred_result=forward(X)
  show_result(X.T,y.T,np.round(pred_result).T)

